{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.2641509433962264,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01509433962264151,
      "grad_norm": 1.1885756254196167,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.4344,
      "step": 10
    },
    {
      "epoch": 0.03018867924528302,
      "grad_norm": 1.2350711822509766,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.5202,
      "step": 20
    },
    {
      "epoch": 0.045283018867924525,
      "grad_norm": 1.0889670848846436,
      "learning_rate": 6e-06,
      "loss": 3.3133,
      "step": 30
    },
    {
      "epoch": 0.06037735849056604,
      "grad_norm": 1.0560632944107056,
      "learning_rate": 7.800000000000002e-06,
      "loss": 3.5811,
      "step": 40
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 0.9839476346969604,
      "learning_rate": 9.800000000000001e-06,
      "loss": 3.309,
      "step": 50
    },
    {
      "epoch": 0.09056603773584905,
      "grad_norm": 1.2471646070480347,
      "learning_rate": 1.18e-05,
      "loss": 3.3988,
      "step": 60
    },
    {
      "epoch": 0.10566037735849057,
      "grad_norm": 1.4643008708953857,
      "learning_rate": 1.38e-05,
      "loss": 3.3136,
      "step": 70
    },
    {
      "epoch": 0.12075471698113208,
      "grad_norm": 1.6947944164276123,
      "learning_rate": 1.58e-05,
      "loss": 3.3671,
      "step": 80
    },
    {
      "epoch": 0.13584905660377358,
      "grad_norm": 1.3193306922912598,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 3.3889,
      "step": 90
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 1.4110257625579834,
      "learning_rate": 1.98e-05,
      "loss": 3.4041,
      "step": 100
    },
    {
      "epoch": 0.1660377358490566,
      "grad_norm": 1.5571174621582031,
      "learning_rate": 1.9998876266681585e-05,
      "loss": 3.1492,
      "step": 110
    },
    {
      "epoch": 0.1811320754716981,
      "grad_norm": NaN,
      "learning_rate": 1.9995505319281645e-05,
      "loss": 3.1866,
      "step": 120
    },
    {
      "epoch": 0.19622641509433963,
      "grad_norm": 1.6833077669143677,
      "learning_rate": 1.998912513923959e-05,
      "loss": 3.1345,
      "step": 130
    },
    {
      "epoch": 0.21132075471698114,
      "grad_norm": 2.39982271194458,
      "learning_rate": 1.9979973338598603e-05,
      "loss": 3.2713,
      "step": 140
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 3.6951653957366943,
      "learning_rate": 1.9968052456652048e-05,
      "loss": 3.3509,
      "step": 150
    },
    {
      "epoch": 0.24150943396226415,
      "grad_norm": 2.836162805557251,
      "learning_rate": 1.995336580101311e-05,
      "loss": 3.3496,
      "step": 160
    },
    {
      "epoch": 0.25660377358490566,
      "grad_norm": 2.903555154800415,
      "learning_rate": 1.9935917446697038e-05,
      "loss": 3.1301,
      "step": 170
    },
    {
      "epoch": 0.27169811320754716,
      "grad_norm": 2.066114902496338,
      "learning_rate": 1.9915712234990486e-05,
      "loss": 3.0433,
      "step": 180
    },
    {
      "epoch": 0.28679245283018867,
      "grad_norm": 2.672436475753784,
      "learning_rate": 1.989275577210824e-05,
      "loss": 3.2602,
      "step": 190
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 3.4455621242523193,
      "learning_rate": 1.9867054427637667e-05,
      "loss": 3.0112,
      "step": 200
    },
    {
      "epoch": 0.3169811320754717,
      "grad_norm": 2.3382301330566406,
      "learning_rate": 1.983861533277142e-05,
      "loss": 3.0679,
      "step": 210
    },
    {
      "epoch": 0.3320754716981132,
      "grad_norm": 2.453738212585449,
      "learning_rate": 1.980744637832877e-05,
      "loss": 3.0971,
      "step": 220
    },
    {
      "epoch": 0.3471698113207547,
      "grad_norm": 3.3834962844848633,
      "learning_rate": 1.977355621256619e-05,
      "loss": 3.0606,
      "step": 230
    },
    {
      "epoch": 0.3622641509433962,
      "grad_norm": 2.883106231689453,
      "learning_rate": 1.9736954238777793e-05,
      "loss": 3.0313,
      "step": 240
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 3.737057685852051,
      "learning_rate": 1.9697650612686228e-05,
      "loss": 2.8811,
      "step": 250
    },
    {
      "epoch": 0.39245283018867927,
      "grad_norm": 3.411750555038452,
      "learning_rate": 1.9655656239624864e-05,
      "loss": 3.0178,
      "step": 260
    },
    {
      "epoch": 0.4075471698113208,
      "grad_norm": 2.5770952701568604,
      "learning_rate": 1.9610982771511947e-05,
      "loss": 2.9644,
      "step": 270
    },
    {
      "epoch": 0.4226415094339623,
      "grad_norm": 3.2287278175354004,
      "learning_rate": 1.95636426036176e-05,
      "loss": 3.0462,
      "step": 280
    },
    {
      "epoch": 0.4377358490566038,
      "grad_norm": 2.8148162364959717,
      "learning_rate": 1.9513648871124604e-05,
      "loss": 3.2234,
      "step": 290
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 2.7868645191192627,
      "learning_rate": 1.9461015445483843e-05,
      "loss": 3.0512,
      "step": 300
    },
    {
      "epoch": 0.4679245283018868,
      "grad_norm": 3.655825138092041,
      "learning_rate": 1.9405756930565496e-05,
      "loss": 2.8572,
      "step": 310
    },
    {
      "epoch": 0.4830188679245283,
      "grad_norm": 4.717287063598633,
      "learning_rate": 1.934788865860698e-05,
      "loss": 3.0887,
      "step": 320
    },
    {
      "epoch": 0.4981132075471698,
      "grad_norm": 2.7350258827209473,
      "learning_rate": 1.928742668595881e-05,
      "loss": 2.8362,
      "step": 330
    },
    {
      "epoch": 0.5132075471698113,
      "grad_norm": 4.092658996582031,
      "learning_rate": 1.9224387788629547e-05,
      "loss": 3.0171,
      "step": 340
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 3.0891072750091553,
      "learning_rate": 1.9158789457631054e-05,
      "loss": 3.1645,
      "step": 350
    },
    {
      "epoch": 0.5433962264150943,
      "grad_norm": 2.7712008953094482,
      "learning_rate": 1.9090649894125395e-05,
      "loss": 2.9809,
      "step": 360
    },
    {
      "epoch": 0.5584905660377358,
      "grad_norm": 3.6965339183807373,
      "learning_rate": 1.9019988004374645e-05,
      "loss": 3.0719,
      "step": 370
    },
    {
      "epoch": 0.5735849056603773,
      "grad_norm": 2.6572704315185547,
      "learning_rate": 1.89468233944951e-05,
      "loss": 2.8661,
      "step": 380
    },
    {
      "epoch": 0.5886792452830188,
      "grad_norm": 4.885863304138184,
      "learning_rate": 1.8871176365017293e-05,
      "loss": 2.964,
      "step": 390
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 3.300893545150757,
      "learning_rate": 1.8793067905253318e-05,
      "loss": 3.0611,
      "step": 400
    },
    {
      "epoch": 0.6188679245283019,
      "grad_norm": 2.635035514831543,
      "learning_rate": 1.8712519687473075e-05,
      "loss": 2.8931,
      "step": 410
    },
    {
      "epoch": 0.6339622641509434,
      "grad_norm": 1.9647024869918823,
      "learning_rate": 1.8629554060890982e-05,
      "loss": 2.8721,
      "step": 420
    },
    {
      "epoch": 0.6490566037735849,
      "grad_norm": 2.813978910446167,
      "learning_rate": 1.8544194045464888e-05,
      "loss": 2.9689,
      "step": 430
    },
    {
      "epoch": 0.6641509433962264,
      "grad_norm": 3.058732271194458,
      "learning_rate": 1.845646332550886e-05,
      "loss": 2.9281,
      "step": 440
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 3.447275400161743,
      "learning_rate": 1.8366386243121654e-05,
      "loss": 2.8767,
      "step": 450
    },
    {
      "epoch": 0.6943396226415094,
      "grad_norm": 2.229013204574585,
      "learning_rate": 1.827398779143265e-05,
      "loss": 2.8377,
      "step": 460
    },
    {
      "epoch": 0.7094339622641509,
      "grad_norm": 2.749011278152466,
      "learning_rate": 1.8179293607667177e-05,
      "loss": 2.8413,
      "step": 470
    },
    {
      "epoch": 0.7245283018867924,
      "grad_norm": 3.122678279876709,
      "learning_rate": 1.8082329966033105e-05,
      "loss": 2.8717,
      "step": 480
    },
    {
      "epoch": 0.7396226415094339,
      "grad_norm": 2.3246729373931885,
      "learning_rate": 1.7983123770430696e-05,
      "loss": 2.8514,
      "step": 490
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 2.8050663471221924,
      "learning_rate": 1.788170254698776e-05,
      "loss": 3.1168,
      "step": 500
    },
    {
      "epoch": 0.769811320754717,
      "grad_norm": 3.0729641914367676,
      "learning_rate": 1.777809443642214e-05,
      "loss": 2.8777,
      "step": 510
    },
    {
      "epoch": 0.7849056603773585,
      "grad_norm": 3.426440477371216,
      "learning_rate": 1.7672328186233692e-05,
      "loss": 2.9215,
      "step": 520
    },
    {
      "epoch": 0.8,
      "grad_norm": 5.388954162597656,
      "learning_rate": 1.7564433142727882e-05,
      "loss": 2.7668,
      "step": 530
    },
    {
      "epoch": 0.8150943396226416,
      "grad_norm": 3.94571852684021,
      "learning_rate": 1.7454439242873257e-05,
      "loss": 2.916,
      "step": 540
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 2.534903049468994,
      "learning_rate": 1.7342377005995014e-05,
      "loss": 2.9708,
      "step": 550
    },
    {
      "epoch": 0.8452830188679246,
      "grad_norm": 4.124805927276611,
      "learning_rate": 1.7228277525307007e-05,
      "loss": 2.8518,
      "step": 560
    },
    {
      "epoch": 0.8603773584905661,
      "grad_norm": 3.062375545501709,
      "learning_rate": 1.7112172459284478e-05,
      "loss": 3.0404,
      "step": 570
    },
    {
      "epoch": 0.8754716981132076,
      "grad_norm": 3.0190913677215576,
      "learning_rate": 1.699409402288001e-05,
      "loss": 2.858,
      "step": 580
    },
    {
      "epoch": 0.8905660377358491,
      "grad_norm": 4.347493648529053,
      "learning_rate": 1.6874074978585018e-05,
      "loss": 3.0749,
      "step": 590
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 4.1637864112854,
      "learning_rate": 1.675214862733935e-05,
      "loss": 2.8813,
      "step": 600
    },
    {
      "epoch": 0.9207547169811321,
      "grad_norm": 3.1214890480041504,
      "learning_rate": 1.662834879929149e-05,
      "loss": 2.892,
      "step": 610
    },
    {
      "epoch": 0.9358490566037736,
      "grad_norm": 2.9830732345581055,
      "learning_rate": 1.6502709844411907e-05,
      "loss": 2.9273,
      "step": 620
    },
    {
      "epoch": 0.9509433962264151,
      "grad_norm": 3.567172050476074,
      "learning_rate": 1.6375266622962188e-05,
      "loss": 2.9759,
      "step": 630
    },
    {
      "epoch": 0.9660377358490566,
      "grad_norm": 3.3754875659942627,
      "learning_rate": 1.6246054495822575e-05,
      "loss": 2.8675,
      "step": 640
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 3.9842021465301514,
      "learning_rate": 1.6115109314680603e-05,
      "loss": 2.9945,
      "step": 650
    },
    {
      "epoch": 0.9962264150943396,
      "grad_norm": 2.789118528366089,
      "learning_rate": 1.5982467412083543e-05,
      "loss": 2.8478,
      "step": 660
    },
    {
      "epoch": 1.0113207547169811,
      "grad_norm": 2.7381956577301025,
      "learning_rate": 1.5848165591357458e-05,
      "loss": 2.8437,
      "step": 670
    },
    {
      "epoch": 1.0264150943396226,
      "grad_norm": 2.976710796356201,
      "learning_rate": 1.571224111639559e-05,
      "loss": 2.7065,
      "step": 680
    },
    {
      "epoch": 1.0415094339622641,
      "grad_norm": 3.2014377117156982,
      "learning_rate": 1.5574731701318987e-05,
      "loss": 2.8332,
      "step": 690
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 3.9127449989318848,
      "learning_rate": 1.5435675500012212e-05,
      "loss": 2.833,
      "step": 700
    },
    {
      "epoch": 1.0716981132075472,
      "grad_norm": 4.453546047210693,
      "learning_rate": 1.5295111095536997e-05,
      "loss": 2.815,
      "step": 710
    },
    {
      "epoch": 1.0867924528301887,
      "grad_norm": 3.510098457336426,
      "learning_rate": 1.5153077489426865e-05,
      "loss": 2.9534,
      "step": 720
    },
    {
      "epoch": 1.1018867924528302,
      "grad_norm": 3.292039155960083,
      "learning_rate": 1.500961409086559e-05,
      "loss": 2.823,
      "step": 730
    },
    {
      "epoch": 1.1169811320754717,
      "grad_norm": 4.0165886878967285,
      "learning_rate": 1.48647607057526e-05,
      "loss": 2.9232,
      "step": 740
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 4.248000144958496,
      "learning_rate": 1.4718557525658272e-05,
      "loss": 2.9255,
      "step": 750
    },
    {
      "epoch": 1.1471698113207547,
      "grad_norm": 2.863025426864624,
      "learning_rate": 1.4571045116672219e-05,
      "loss": 2.784,
      "step": 760
    },
    {
      "epoch": 1.1622641509433962,
      "grad_norm": 3.7821919918060303,
      "learning_rate": 1.4422264408147676e-05,
      "loss": 2.8092,
      "step": 770
    },
    {
      "epoch": 1.1773584905660377,
      "grad_norm": 3.1115431785583496,
      "learning_rate": 1.4272256681345087e-05,
      "loss": 2.8914,
      "step": 780
    },
    {
      "epoch": 1.1924528301886792,
      "grad_norm": 3.034912586212158,
      "learning_rate": 1.4121063557978051e-05,
      "loss": 3.0514,
      "step": 790
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 2.980736255645752,
      "learning_rate": 1.3968726988664788e-05,
      "loss": 2.9026,
      "step": 800
    },
    {
      "epoch": 1.2226415094339622,
      "grad_norm": 5.576872825622559,
      "learning_rate": 1.3815289241288383e-05,
      "loss": 2.7995,
      "step": 810
    },
    {
      "epoch": 1.2377358490566037,
      "grad_norm": 2.893413782119751,
      "learning_rate": 1.3660792889268967e-05,
      "loss": 2.74,
      "step": 820
    },
    {
      "epoch": 1.2528301886792452,
      "grad_norm": 2.911766767501831,
      "learning_rate": 1.3505280799751134e-05,
      "loss": 2.7877,
      "step": 830
    },
    {
      "epoch": 1.2679245283018867,
      "grad_norm": 2.442986249923706,
      "learning_rate": 1.3348796121709862e-05,
      "loss": 2.8021,
      "step": 840
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 3.1950411796569824,
      "learning_rate": 1.3191382273978237e-05,
      "loss": 2.806,
      "step": 850
    },
    {
      "epoch": 1.2981132075471697,
      "grad_norm": 3.254237651824951,
      "learning_rate": 1.3033082933200287e-05,
      "loss": 2.9809,
      "step": 860
    },
    {
      "epoch": 1.3132075471698113,
      "grad_norm": 4.456250190734863,
      "learning_rate": 1.287394202171232e-05,
      "loss": 2.9445,
      "step": 870
    },
    {
      "epoch": 1.3283018867924528,
      "grad_norm": 3.561894416809082,
      "learning_rate": 1.2714003695356037e-05,
      "loss": 2.793,
      "step": 880
    },
    {
      "epoch": 1.3433962264150943,
      "grad_norm": 2.9067184925079346,
      "learning_rate": 1.2553312331226896e-05,
      "loss": 2.7256,
      "step": 890
    },
    {
      "epoch": 1.3584905660377358,
      "grad_norm": 2.910029649734497,
      "learning_rate": 1.2391912515361085e-05,
      "loss": 2.8695,
      "step": 900
    },
    {
      "epoch": 1.3735849056603773,
      "grad_norm": 3.2427783012390137,
      "learning_rate": 1.2229849030364496e-05,
      "loss": 2.9325,
      "step": 910
    },
    {
      "epoch": 1.388679245283019,
      "grad_norm": 3.1974518299102783,
      "learning_rate": 1.2067166842987175e-05,
      "loss": 2.7625,
      "step": 920
    },
    {
      "epoch": 1.4037735849056605,
      "grad_norm": 3.8276729583740234,
      "learning_rate": 1.1903911091646684e-05,
      "loss": 3.0427,
      "step": 930
    },
    {
      "epoch": 1.418867924528302,
      "grad_norm": 3.518165111541748,
      "learning_rate": 1.1740127073903826e-05,
      "loss": 2.9714,
      "step": 940
    },
    {
      "epoch": 1.4339622641509435,
      "grad_norm": 2.772460460662842,
      "learning_rate": 1.1575860233894195e-05,
      "loss": 2.8354,
      "step": 950
    },
    {
      "epoch": 1.449056603773585,
      "grad_norm": 3.654085397720337,
      "learning_rate": 1.1411156149719094e-05,
      "loss": 2.9474,
      "step": 960
    },
    {
      "epoch": 1.4641509433962265,
      "grad_norm": 3.1216819286346436,
      "learning_rate": 1.1246060520799244e-05,
      "loss": 2.8592,
      "step": 970
    },
    {
      "epoch": 1.479245283018868,
      "grad_norm": 3.748459577560425,
      "learning_rate": 1.1080619155194873e-05,
      "loss": 2.8383,
      "step": 980
    },
    {
      "epoch": 1.4943396226415095,
      "grad_norm": 3.4247710704803467,
      "learning_rate": 1.0914877956895604e-05,
      "loss": 2.9052,
      "step": 990
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 3.033480167388916,
      "learning_rate": 1.0748882913083794e-05,
      "loss": 2.9445,
      "step": 1000
    },
    {
      "epoch": 1.5245283018867926,
      "grad_norm": 3.311739921569824,
      "learning_rate": 1.0582680081374728e-05,
      "loss": 2.8305,
      "step": 1010
    },
    {
      "epoch": 1.539622641509434,
      "grad_norm": 3.130091667175293,
      "learning_rate": 1.041631557703732e-05,
      "loss": 2.8735,
      "step": 1020
    },
    {
      "epoch": 1.5547169811320756,
      "grad_norm": 3.0723376274108887,
      "learning_rate": 1.0249835560198772e-05,
      "loss": 2.795,
      "step": 1030
    },
    {
      "epoch": 1.569811320754717,
      "grad_norm": 2.7382144927978516,
      "learning_rate": 1.0083286223036845e-05,
      "loss": 2.7989,
      "step": 1040
    },
    {
      "epoch": 1.5849056603773586,
      "grad_norm": 3.0529019832611084,
      "learning_rate": 9.916713776963156e-06,
      "loss": 2.7907,
      "step": 1050
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.838012933731079,
      "learning_rate": 9.75016443980123e-06,
      "loss": 2.6343,
      "step": 1060
    },
    {
      "epoch": 1.6150943396226416,
      "grad_norm": 3.022716760635376,
      "learning_rate": 9.583684422962686e-06,
      "loss": 2.7388,
      "step": 1070
    },
    {
      "epoch": 1.630188679245283,
      "grad_norm": 3.643824338912964,
      "learning_rate": 9.417319918625274e-06,
      "loss": 2.9897,
      "step": 1080
    },
    {
      "epoch": 1.6452830188679246,
      "grad_norm": 4.429660797119141,
      "learning_rate": 9.251117086916209e-06,
      "loss": 2.7951,
      "step": 1090
    },
    {
      "epoch": 1.6603773584905661,
      "grad_norm": 3.727337121963501,
      "learning_rate": 9.0851220431044e-06,
      "loss": 2.7891,
      "step": 1100
    },
    {
      "epoch": 1.6754716981132076,
      "grad_norm": 3.16579270362854,
      "learning_rate": 8.919380844805129e-06,
      "loss": 2.8601,
      "step": 1110
    },
    {
      "epoch": 1.6905660377358491,
      "grad_norm": 2.9185004234313965,
      "learning_rate": 8.753939479200758e-06,
      "loss": 2.9057,
      "step": 1120
    },
    {
      "epoch": 1.7056603773584906,
      "grad_norm": 2.8881642818450928,
      "learning_rate": 8.588843850280911e-06,
      "loss": 2.8185,
      "step": 1130
    },
    {
      "epoch": 1.7207547169811321,
      "grad_norm": 3.478705406188965,
      "learning_rate": 8.424139766105808e-06,
      "loss": 2.8839,
      "step": 1140
    },
    {
      "epoch": 1.7358490566037736,
      "grad_norm": 4.768740653991699,
      "learning_rate": 8.259872926096177e-06,
      "loss": 2.7337,
      "step": 1150
    },
    {
      "epoch": 1.7509433962264151,
      "grad_norm": 3.0085549354553223,
      "learning_rate": 8.096088908353316e-06,
      "loss": 2.9175,
      "step": 1160
    },
    {
      "epoch": 1.7660377358490567,
      "grad_norm": 3.6819896697998047,
      "learning_rate": 7.932833157012829e-06,
      "loss": 2.733,
      "step": 1170
    },
    {
      "epoch": 1.7811320754716982,
      "grad_norm": 3.0391085147857666,
      "learning_rate": 7.770150969635509e-06,
      "loss": 2.8588,
      "step": 1180
    },
    {
      "epoch": 1.7962264150943397,
      "grad_norm": 2.665266513824463,
      "learning_rate": 7.608087484638915e-06,
      "loss": 2.7543,
      "step": 1190
    },
    {
      "epoch": 1.8113207547169812,
      "grad_norm": 3.3180923461914062,
      "learning_rate": 7.446687668773105e-06,
      "loss": 2.7998,
      "step": 1200
    },
    {
      "epoch": 1.8264150943396227,
      "grad_norm": 4.480628490447998,
      "learning_rate": 7.2859963046439665e-06,
      "loss": 2.8047,
      "step": 1210
    },
    {
      "epoch": 1.8415094339622642,
      "grad_norm": 2.617852210998535,
      "learning_rate": 7.12605797828768e-06,
      "loss": 2.8257,
      "step": 1220
    },
    {
      "epoch": 1.8566037735849057,
      "grad_norm": 2.7800381183624268,
      "learning_rate": 6.966917066799714e-06,
      "loss": 2.8984,
      "step": 1230
    },
    {
      "epoch": 1.8716981132075472,
      "grad_norm": 2.666006088256836,
      "learning_rate": 6.8086177260217675e-06,
      "loss": 2.9715,
      "step": 1240
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 3.273223400115967,
      "learning_rate": 6.651203878290139e-06,
      "loss": 2.8305,
      "step": 1250
    },
    {
      "epoch": 1.9018867924528302,
      "grad_norm": 3.0963363647460938,
      "learning_rate": 6.494719200248867e-06,
      "loss": 2.8363,
      "step": 1260
    },
    {
      "epoch": 1.9169811320754717,
      "grad_norm": 2.9978058338165283,
      "learning_rate": 6.339207110731036e-06,
      "loss": 2.747,
      "step": 1270
    },
    {
      "epoch": 1.9320754716981132,
      "grad_norm": 3.576516628265381,
      "learning_rate": 6.184710758711616e-06,
      "loss": 2.7125,
      "step": 1280
    },
    {
      "epoch": 1.9471698113207547,
      "grad_norm": 4.726477146148682,
      "learning_rate": 6.046567933165552e-06,
      "loss": 2.6817,
      "step": 1290
    },
    {
      "epoch": 1.9622641509433962,
      "grad_norm": 3.3430144786834717,
      "learning_rate": 5.894119338857671e-06,
      "loss": 2.8032,
      "step": 1300
    },
    {
      "epoch": 1.9773584905660377,
      "grad_norm": 3.151089906692505,
      "learning_rate": 5.7428099777918e-06,
      "loss": 2.8473,
      "step": 1310
    },
    {
      "epoch": 1.9924528301886792,
      "grad_norm": 2.053209066390991,
      "learning_rate": 5.592681832837913e-06,
      "loss": 2.802,
      "step": 1320
    },
    {
      "epoch": 2.0075471698113208,
      "grad_norm": 3.5308194160461426,
      "learning_rate": 5.443776559121279e-06,
      "loss": 2.8623,
      "step": 1330
    },
    {
      "epoch": 2.0226415094339623,
      "grad_norm": 3.503237009048462,
      "learning_rate": 5.296135472464686e-06,
      "loss": 2.8039,
      "step": 1340
    },
    {
      "epoch": 2.0377358490566038,
      "grad_norm": 2.857888698577881,
      "learning_rate": 5.149799537924749e-06,
      "loss": 2.7022,
      "step": 1350
    },
    {
      "epoch": 2.0528301886792453,
      "grad_norm": 9.455846786499023,
      "learning_rate": 5.004809358425639e-06,
      "loss": 2.8212,
      "step": 1360
    },
    {
      "epoch": 2.0679245283018868,
      "grad_norm": 3.797384738922119,
      "learning_rate": 4.861205163493229e-06,
      "loss": 2.9806,
      "step": 1370
    },
    {
      "epoch": 2.0830188679245283,
      "grad_norm": 3.5838146209716797,
      "learning_rate": 4.719026798092838e-06,
      "loss": 2.7318,
      "step": 1380
    },
    {
      "epoch": 2.09811320754717,
      "grad_norm": 4.100687026977539,
      "learning_rate": 4.5783137115737e-06,
      "loss": 2.8756,
      "step": 1390
    },
    {
      "epoch": 2.1132075471698113,
      "grad_norm": 4.019829750061035,
      "learning_rate": 4.439104946723228e-06,
      "loss": 2.7663,
      "step": 1400
    },
    {
      "epoch": 2.128301886792453,
      "grad_norm": 4.0828142166137695,
      "learning_rate": 4.301439128934015e-06,
      "loss": 2.8918,
      "step": 1410
    },
    {
      "epoch": 2.1433962264150943,
      "grad_norm": 4.52817440032959,
      "learning_rate": 4.165354455486707e-06,
      "loss": 2.7983,
      "step": 1420
    },
    {
      "epoch": 2.158490566037736,
      "grad_norm": 3.9604203701019287,
      "learning_rate": 4.030888684951638e-06,
      "loss": 2.8057,
      "step": 1430
    },
    {
      "epoch": 2.1735849056603773,
      "grad_norm": 4.44850492477417,
      "learning_rate": 3.898079126712184e-06,
      "loss": 2.8137,
      "step": 1440
    },
    {
      "epoch": 2.188679245283019,
      "grad_norm": 3.4175899028778076,
      "learning_rate": 3.766962630612785e-06,
      "loss": 2.8774,
      "step": 1450
    },
    {
      "epoch": 2.2037735849056603,
      "grad_norm": 4.300398349761963,
      "learning_rate": 3.6375755767344047e-06,
      "loss": 2.9358,
      "step": 1460
    },
    {
      "epoch": 2.218867924528302,
      "grad_norm": 3.4831395149230957,
      "learning_rate": 3.509953865300414e-06,
      "loss": 2.9049,
      "step": 1470
    },
    {
      "epoch": 2.2339622641509433,
      "grad_norm": 4.722700595855713,
      "learning_rate": 3.3841329067155693e-06,
      "loss": 2.8195,
      "step": 1480
    },
    {
      "epoch": 2.249056603773585,
      "grad_norm": 3.61657977104187,
      "learning_rate": 3.2601476117408937e-06,
      "loss": 2.6611,
      "step": 1490
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 3.7752439975738525,
      "learning_rate": 3.1380323818072155e-06,
      "loss": 2.7176,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1986,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.38456399347712e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
